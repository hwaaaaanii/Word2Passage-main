import random
import json
import argparse
import re, os, ast
import numpy as np
import torch
from tqdm import tqdm
import subprocess

# -----------------------------------------------------------------------------
# 1) Imports and initial setup
#    - Fix random seeds for reproducibility
# -----------------------------------------------------------------------------
from llm_module.prompt_templates import *



# -----------------------------------------------------------------------------
# 2) Utility functions for parsing and generating outputs
#    - parse_response
#    - generate
#    - generate_w_repetition
#    - generate_w2P
# -----------------------------------------------------------------------------
def parse_response(data_str: str) -> dict:
    """
    Convert a model's string output (JSON-like) into a Python dictionary.
    Expected keys: 'word', 'sentence', 'passage'.
    Removes line breaks and uses 'ast.literal_eval' to parse the string safely.
    """
    data_str = data_str.strip()
    data_str = data_str.replace('\n', ' ')
    data_dict = ast.literal_eval(data_str)
    output = {
        'words': data_dict['word'],
        'sentence': data_dict['sentence'],
        'passage': data_dict['passage'],
    }
    return output



def generate(prompt: str, generate_func) -> str:
    """
    Use the provided 'generate_func' (LLM generation function) to generate output
    from a single prompt. Returns the raw string generated by the model.
    """
    output = generate_func(prompt)
    return output



def generate_w_repetition(prompt: str, generate_func, max_retries: int = 5) -> list:
    """
    Generate multiple outputs from the same prompt by calling 'generate_func'
    up to 'max_retries' times. Returns a list of outputs.
    """
    final_output = []
    count = 0
    while count < max_retries:
        output = generate_func(prompt)
        final_output.append(output)
        count += 1
    return final_output



def generate_w2P(prompt: str, generate_func, parse_output_func, max_retries: int = 20, num_of_generation: int = 5) -> list:
    """
    Repeatedly generate outputs for the W2P approach, parse them into a dictionary,
    and ensure they contain the expected keys ('word', 'sentence', 'passage').
    Stops if we successfully parse 5 valid outputs or exceed 'max_retries'.
    """
    final_output = []
    required_keys = ['words', 'sentence', 'passage']
    count = 0

    while count < max_retries:
        try:
            output = generate_func(prompt)
            # print(output)
            output = output.replace('{', '').replace('}','')
            output = '{'+output+'}'
            parsed_output = parse_output_func(output)
            if all(key in parsed_output for key in required_keys):
                final_output.append(parsed_output)
            
        except Exception as e:
            print(f"Parsing failed with error: {e}")
        count += 1
        if len(final_output) >= num_of_generation:
            break
    return final_output



# -----------------------------------------------------------------------------
# 3) pseudo_generator: main function to generate and store pseudo references
#    - Takes a list of queries and an output_path
#    - Uses multiple rewriting prompts (HyDE, MuGI, W2P) to generate pseudo docs
# -----------------------------------------------------------------------------
def pseudo_generator(queries: list, output_path: str):
    """
    For each query, generate multiple pseudo references (HyDE, MuGI, W2P).
    Then write these references, along with a query classification, to a JSON file.
    """
    output = []
    error = 0
    
    output_dir = os.path.dirname(output_path)
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    
    for idx, query in enumerate(tqdm(queries, total=len(queries), desc="Processing Queries", unit="query")):
        dic, parsed_output = {}, {}
        HyDE_prompt = HyDE_passage_prompt(query)
        MuGI_prompt = MuGI_passage_prompt(query)
        W2P_prompt = W2P_generate_prompt(query)

        generated = generate_w2P(prompt=W2P_prompt, generate_func=generate_W2P, parse_output_func=parse_response)
        parsed_output['W2P'] = generated
        print(f'Word2Passage Output')
        print(generated)
        print('-'*50)
        
        # generated = generate(prompt=HyDE_prompt, generate_func=generate_HyDE)
        # parsed_output['HyDE'] = generated
        # print(f'HyDE Output')
        # print(generated)
        # print('-'*50)
        
        # generated = generate_w_repetition(prompt=MuGI_prompt, generate_func=generate_MuGI)
        # parsed_output['MuGI'] = generated
        # print(f'MuGI Output')
        # print(generated)
        # print('-'*50)
        
        if None in parsed_output.values():
            error += 1
            
        if idx == 0:
            print(f'==============Samples==============')
            print(f'Query : {query}\n\n')
            print(f'Pseudo docs : {parsed_output}')
            print('====================================')

        dic[query] = [parsed_output]
        output.append(dic)
    with open(output_path, 'w', encoding='utf-8', errors='replace') as outfile:
        json.dump(output, outfile, ensure_ascii=False, indent=4)



def label_query_type(output_path):
    """
    Reads a JSON list of query objects from the given file,
    classifies each query,
    then writes back to the same file with the added 'query_type' field.
    """
    set_seed()
    output = []
    with open(output_path, 'r', encoding='utf-8') as reader:
        reference_data = json.load(reader)
    
        for entry in reference_data:
            query = list(entry.keys())[0]
            parsed_output = entry[query]
            query_type_prompt = query_type_classification_prompt(query)
            classification_result = generate(prompt=query_type_prompt, 
                                             generate_func=generate_query_label)
            classified_type = re.findall(r"Query Type: (\w+)", classification_result)
            print(f"Query: {query} | Classified Type: {classified_type}")
            entry[query] = [
                {'query_type': classified_type[0]},
                parsed_output[0]
            ]
            output.append(entry)

    with open(output_path, 'w', encoding='utf-8', errors='replace') as outfile:
        json.dump(output, outfile, ensure_ascii=False, indent=4)

        
        
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate pseudo passages")
    parser.add_argument('-dataset', type=str, required=True, help="Specify the dataset")
    parser.add_argument('-task', type=str, choices=['IR', 'QA'], required=True, help="Specify the task: IR or QA")
    parser.add_argument('-LLM', type=str, required=True)
    args = parser.parse_args()

    if args.LLM.startswith('Qwen2.5'):
        from llm_module.qwen_model import *
        set_seed()
        load_qwen_model(args.LLM) 

    elif args.LLM.startswith('Llama3.1'):
        from llm_module.llama_model import *
        set_seed()
        load_llama_model(args.LLM)
    else:
        raise ValueError(f"Unsupported LLM option: {args.LLM}")


    qa_path = f'./datasets/{args.task}/{args.dataset}/{args.dataset}/test_subsample_processed.json'
    output_path = f'./datasets/{args.task}/{args.dataset}/{args.dataset}/pseudo_references/{args.LLM}_{args.dataset}_pseudo_references.json'
    queries = []
    with open(qa_path, 'r', encoding='utf-8') as reader:
        for line in reader:
            data = json.loads(line)
            queries.append(data['query_text'])
            
    torch.cuda.empty_cache()
    pseudo_generator(queries, output_path)
    from llm_module.llama_model import *
    load_llama_model('Llama3.1_8b')
    label_query_type(output_path)